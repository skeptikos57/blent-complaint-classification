"""Script d'entra√Ænement du mod√®le de classification de plaintes pour une compagnie d'assurance.

Ce script :
1. Charge un dataset de plaintes clients avec leurs cat√©gories de produits
2. Transforme les textes en vecteurs num√©riques avec Word2Vec
3. Entra√Æne un r√©seau de neurones (CNN + LSTM) pour pr√©dire la cat√©gorie
4. Sauvegarde les mod√®les entra√Æn√©s pour une utilisation ult√©rieure
"""

import os
from dotenv import load_dotenv

# Charger les variables d'environnement avec override=True pour forcer le rechargement
load_dotenv(override=True)

# Configuration pour r√©duire les messages d'information de TensorFlow
# IMPORTANT : Ces lignes DOIVENT √™tre avant l'import de TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # 3 = Afficher seulement les erreurs (pas les warnings/infos)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # D√©sactive les optimisations oneDNN (r√©duit les messages)

# Imports des biblioth√®ques n√©cessaires
import numpy as np  # Pour les calculs num√©riques et les tableaux
import pandas as pd  # Pour manipuler les donn√©es sous forme de tableaux
import matplotlib.pyplot as plt  # Pour cr√©er des graphiques (pas utilis√© actuellement)
import tensorflow as tf  # Framework de deep learning pour cr√©er le r√©seau de neurones

# Imports des modules sp√©cifiques
from dotenv import load_dotenv  # Pour charger les variables d'environnement depuis le fichier .env
from gensim.models import Word2Vec  # Pour cr√©er des embeddings de mots (Word2Vec)
from keras import layers  # Pour construire les couches du r√©seau de neurones
from spacy.tokenizer import Tokenizer  # Pour d√©couper le texte (pas utilis√© actuellement)
from spacy.lang.en import English  # Pour le traitement du texte en anglais
from sklearn.model_selection import train_test_split  # Pour diviser les donn√©es en train/test
from sklearn.utils.class_weight import compute_class_weight  # Pour g√©rer le d√©s√©quilibre des classes
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint  # Pour le monitoring et la sauvegarde
import json  # Pour lire le fichier d'analyse des classes

# Configuration des hyperparam√®tres (avec valeurs par d√©faut si non d√©finies)
W2V_SIZE = int(os.getenv("W2V_SIZE", 100))  # Dimension des vecteurs Word2Vec
W2V_MIN_COUNT = int(os.getenv("W2V_MIN_COUNT", 3))  # Fr√©quence minimale des mots
MAX_LENGTH = int(os.getenv("MAX_LENGTH", 64))  # Nombre max de mots par commentaire
NB_COMMENT = int(os.getenv("NB_COMMENT", 10000))  # Nombre de commentaires √† utiliser

def tokenize_corpus(texts):
    """Transforme une liste de textes de plaintes en liste de mots (tokens).
    
    Cette fonction pr√©pare le texte pour Word2Vec :
    - Met tout en minuscules pour uniformiser
    - D√©coupe chaque texte en mots individuels
    - Enl√®ve la ponctuation (virgules, points, etc.)
    
    Args:
        texts: Liste de textes de plaintes (textes bruts)
    
    Returns:
        Liste de listes de mots. Chaque sous-liste = les mots d'une plainte
        Exemple: [["credit", "report", "error"], ["mortgage", "payment", "issue"]]
    """
    tokens = []  # Liste qui contiendra tous les textes tokenis√©s
    nlp = English()  # Cr√©er un objet pour analyser l'anglais
    
    for text in texts:
        if pd.isna(text):  # G√©rer les valeurs manquantes
            text = ""
        text = str(text).lower()  # Convertir en minuscules
        # nlp(text) d√©coupe le texte et analyse chaque mot
        # x.text r√©cup√®re le mot, x.is_punct v√©rifie si c'est de la ponctuation
        tokens.append([x.text for x in nlp(text) if not x.is_punct])
    return tokens

def fit_word2vec(tokens):
    """Entra√Æne un mod√®le Word2Vec pour transformer les mots en vecteurs num√©riques.
    
    Word2Vec apprend √† repr√©senter chaque mot comme un vecteur de nombres.
    Les mots similaires auront des vecteurs proches (ex: "excellent" et "super").
    Version optimis√©e pour les plaintes financi√®res (vocabulaire technique).
    
    Args:
        tokens: Liste de listes de mots provenant de tokenize_corpus
    
    Returns:
        Mod√®le Word2Vec entra√Æn√©
    """
    import multiprocessing
    
    # Cr√©ation et entra√Ænement du mod√®le Word2Vec avec param√®tres optimis√©s
    w2v = Word2Vec(
        sentences=tokens,           # Les commentaires tokenis√©s
        vector_size=W2V_SIZE,        # Taille des vecteurs (200 recommand√©)
        min_count=W2V_MIN_COUNT,     # Ignore les mots rares (3 est bon)
        window=7,                    # Augment√© de 5 √† 7 pour mieux capturer le contexte financier
        sg=1,                        # Skip-gram (1) au lieu de CBOW (0) - meilleur pour vocabulaire technique
        hs=0,                        # Hierarchical softmax d√©sactiv√©
        negative=10,                 # Negative sampling augment√© de 5 √† 10 pour meilleure pr√©cision
        alpha=0.025,                 # Taux d'apprentissage initial
        min_alpha=0.0001,           # Taux d'apprentissage minimal
        epochs=15,                   # Augment√© de 5 √† 15 pour meilleur apprentissage (√©tait implicite)
        workers=multiprocessing.cpu_count() - 1,  # Utilise tous les CPU disponibles - 1
        seed=42                      # Pour la reproductibilit√©
    )
    
    # Sauvegarder le mod√®le pour pouvoir le r√©utiliser plus tard
    w2v.wv.save("models/w2v.wv")
    
    # Afficher des statistiques sur le vocabulaire appris
    print(f"\nüìä Informations sur le mod√®le Word2Vec:")
    print(f"  ‚Ä¢ Taille du vocabulaire: {len(w2v.wv)} mots uniques")
    print(f"  ‚Ä¢ Dimensions des vecteurs: {W2V_SIZE}")
    print(f"  ‚Ä¢ Fen√™tre de contexte: 7 mots")
    print(f"  ‚Ä¢ Algorithme: Skip-gram avec negative sampling")
    print(f"  ‚Ä¢ Epochs d'entra√Ænement: 15")
    
    # Exemples de similarit√©s pour v√©rifier la qualit√©
    try:
        test_words = ['credit', 'payment', 'account', 'debt', 'loan']
        print("\nüîç Test de similarit√© (qualit√© des embeddings):")
        for word in test_words:
            if word in w2v.wv:
                similar = w2v.wv.most_similar(word, topn=3)
                print(f"  ‚Ä¢ '{word}' ‚Üí {[w for w, _ in similar]}")
    except:
        pass
    
    return w2v

def text2vec(tokens, w2v):
    """Convertit chaque texte de plainte en une matrice de vecteurs Word2Vec.
    Version optimis√©e pour grandes quantit√©s de donn√©es.
    
    Transforme les mots en nombres pour que le r√©seau de neurones puisse les comprendre.
    Chaque commentaire devient une matrice de taille fixe (W2V_SIZE x MAX_LENGTH).
    
    Args:
        tokens: Liste de listes de mots
        w2v: Mod√®le Word2Vec entra√Æn√©
    
    Returns:
        Array numpy 3D de forme (nb_commentaires, W2V_SIZE, MAX_LENGTH)
    """
    n_samples = len(tokens)
    
    # Estimation de la m√©moire n√©cessaire
    memory_gb = (n_samples * W2V_SIZE * MAX_LENGTH * 4) / (1024**3)
    print(f"  Vectorisation de {n_samples} textes...")
    print(f"  Dimensions: {W2V_SIZE} x {MAX_LENGTH}")
    print(f"  M√©moire estim√©e: {memory_gb:.2f} GB")
    
    if memory_gb > 16:
        print(f"  ‚ö†Ô∏è  ATTENTION: Utilisation m√©moire tr√®s √©lev√©e!")
        print(f"  Suggestions:")
        print(f"    - R√©duire NB_COMMENT (actuellement {n_samples})")
        print(f"    - R√©duire W2V_SIZE (actuellement {W2V_SIZE})")
        print(f"    - R√©duire MAX_LENGTH (actuellement {MAX_LENGTH})")
    
    # Pr√©-allouer le tableau numpy pour √©conomiser la m√©moire
    # Utilisation de float32 au lieu de float64 pour diviser la m√©moire par 2
    X = np.zeros((n_samples, W2V_SIZE, MAX_LENGTH), dtype=np.float32)
    
    # Traitement par batch pour meilleure gestion m√©moire
    batch_size = 5000
    for batch_start in range(0, n_samples, batch_size):
        batch_end = min(batch_start + batch_size, n_samples)
        
        # Pour chaque commentaire dans le batch
        for i in range(batch_start, batch_end):
            row_tokens = tokens[i]
            # Remplir la matrice avec les vecteurs des mots (limit√© √† MAX_LENGTH)
            for j in range(min(MAX_LENGTH, len(row_tokens))):
                # V√©rifier si le mot existe dans le vocabulaire
                if row_tokens[j] in w2v.wv:
                    # Mettre le vecteur du mot j dans la colonne j
                    X[i, :, j] = w2v.wv[row_tokens[j]]
                # Si le mot n'est pas dans le vocabulaire, on laisse des z√©ros
        
        # Afficher la progression par batch
        progress = (batch_end * 100) / n_samples
        print(f"  {progress:.1f}% effectu√© ({batch_end}/{n_samples} textes)")
    
    print(f"  ‚úì Vectorisation termin√©e : {X.nbytes / 1024**3:.2f} GB utilis√©s")
    
    return X

def prepare_labels(data, class_mapping):
    """Pr√©pare les √©tiquettes (labels) pour l'entra√Ænement.
    
    Transforme les cat√©gories de produits en indices num√©riques puis en one-hot encoding.
    Version optimis√©e pour grandes quantit√©s de donn√©es.
    
    Args:
        data: DataFrame avec colonne 'product'
        class_mapping: Dictionnaire mappant les noms de classes aux indices
    
    Returns:
        Array one-hot encoded (nb_classes colonnes : une seule vaut 1, les autres 0)
    """
    print(f"  Pr√©paration de {len(data)} labels...")
    
    # Mapper les cat√©gories de produits aux indices num√©riques
    y_indices = data['product'].map(class_mapping)
    
    # Version vectoris√©e plus rapide (sans boucle Python)
    num_classes = len(class_mapping)
    n_samples = len(y_indices)
    
    # Utiliser float32 au lieu de float64 pour √©conomiser la m√©moire
    y = np.zeros((n_samples, num_classes), dtype=np.float32)
    
    # Remplir uniquement les indices valides (vectoris√©)
    valid_mask = ~y_indices.isna()
    valid_indices = y_indices[valid_mask].astype(int).values
    valid_positions = np.where(valid_mask)[0]
    
    # Remplissage vectoris√© (beaucoup plus rapide que la boucle)
    y[valid_positions, valid_indices] = 1
    
    print(f"  ‚úì Labels pr√©par√©s : shape {y.shape}, m√©moire : {y.nbytes / 1024**2:.1f} MB")
    
    return y

def create_rnn(num_classes):
    """Cr√©e l'architecture du r√©seau de neurones pour la classification.
    
    Architecture hybride CNN + LSTM :
    - CNN (Convolution) : D√©tecte des patterns locaux dans le texte
    - LSTM : Comprend les s√©quences et le contexte
    - Dense : Couches finales pour la classification
    
    Returns:
        Mod√®le Keras non compil√©
    """
    return tf.keras.Sequential([
        # Couche d'entr√©e : attend des matrices de taille (100, 64)
        layers.Input(shape=(W2V_SIZE, MAX_LENGTH)),
        
        # Convolution 1D : d√©tecte des patterns de 3 mots cons√©cutifs
        # 64 filtres pour mieux capturer la diversit√© des plaintes
        layers.Convolution1D(64, kernel_size=3, padding='same', activation='relu'),
        
        # MaxPooling : r√©duit la taille de moiti√© en gardant les valeurs importantes
        layers.MaxPool1D(2),
        
        # Convolution suppl√©mentaire pour capturer plus de patterns
        layers.Convolution1D(128, kernel_size=3, padding='same', activation='relu'),
        layers.MaxPool1D(2),
        
        # LSTM : r√©seau r√©current qui comprend les s√©quences
        # 256 neurones pour g√©rer la complexit√© accrue
        layers.LSTM(256, activation="tanh", return_sequences=True),
        
        # Dropout : d√©sactive al√©atoirement 20% des neurones (√©vite le surapprentissage)
        layers.Dropout(0.2),
        
        # Deuxi√®me couche LSTM
        layers.LSTM(128, activation="tanh"),
        
        # Dropout suppl√©mentaire
        layers.Dropout(0.2),
        
        # Dense : couche classique avec 128 neurones
        layers.Dense(128, activation="relu"),
        
        # Dropout avant la sortie
        layers.Dropout(0.1),
        
        # Couche de sortie : autant de neurones que de cat√©gories
        # Softmax garantit que la somme des probabilit√©s = 1
        layers.Dense(num_classes, activation="softmax")
    ])
            
def load_class_mapping():
    """Charge le mapping des classes depuis le fichier d'analyse.
    
    Returns:
        tuple: (class_mapping dict, num_classes int, class_weights dict)
    """
    with open('data/prepared/analyse_result.json', 'r', encoding='utf-8') as f:
        analysis = json.load(f)
    
    # Cr√©er le mapping nom de classe -> indice
    class_mapping = {}
    class_weights = {}
    for i, classe in enumerate(analysis['classes']):
        class_mapping[classe['nom']] = i
        # Utiliser l'inverse du pourcentage comme poids pour compenser le d√©s√©quilibre
        # G√©rer les classes avec pourcentage nul ou tr√®s faible
        pourcentage = max(classe['pourcentage'] / 100.0, 0.0001)  # Minimum 0.01%
        class_weights[i] = 1.0 / pourcentage
    
    # Sauvegarder le mapping pour l'utiliser dans predict.py
    with open('models/class_mapping.json', 'w', encoding='utf-8') as f:
        json.dump({
            'class_mapping': class_mapping,
            'index_to_class': {str(v): k for k, v in class_mapping.items()},
            'num_classes': len(class_mapping)
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìà Classes identifi√©es: {len(class_mapping)} cat√©gories")
    print(f"Cat√©gories les plus fr√©quentes:")
    for classe in analysis['classes'][:5]:
        print(f"  - {classe['nom'][:50]}: {classe['pourcentage']:.2f}%")
    
    return class_mapping, len(class_mapping), class_weights

def main():
    """Fonction principale qui orchestre tout l'entra√Ænement."""
    
    # √âTAPE 0 : Charger le mapping des classes
    class_mapping, num_classes, class_weights = load_class_mapping()
    
    # √âTAPE 1 : Chargement des donn√©es
    # Lit le fichier CSV pr√©par√© par prepare_data.py
    data_path = 'data/prepared/complaints_processed.csv'
    if not os.path.exists(data_path):
        print(f"Erreur: Le fichier {data_path} n'existe pas.")
        print("Veuillez d'abord ex√©cuter prepare_data.py pour pr√©parer les donn√©es.")
        return
    
    print(f"\nüìÇ Chargement des donn√©es depuis {data_path}...")
    print(f"Limite configur√©e: {NB_COMMENT} plaintes")
    
    # Charger seulement le nombre n√©cessaire de lignes
    data = pd.read_csv(data_path, nrows=NB_COMMENT)
    
    # V√©rifier les colonnes
    if 'complaint_text' not in data.columns or 'product' not in data.columns:
        print("Erreur: Le fichier CSV doit contenir les colonnes 'complaint_text' et 'product'")
        return
    
    print(f"Donn√©es charg√©es: {len(data)} plaintes")
    
    # √âTAPE 2 : Pr√©paration du texte
    # V√©rifier si on doit utiliser le cache ou reg√©n√©rer
    import pickle
    import sys
    
    use_cache = True
    if len(sys.argv) > 1 and sys.argv[1] == '--regenerate':
        use_cache = False
        print("üîÑ Mode reg√©n√©ration : cr√©ation de nouveaux tokens et Word2Vec")
    
    tokens_cache_path = 'models/tokens_cache.pkl'
    
    if use_cache and os.path.exists(tokens_cache_path) and os.path.exists('models/w2v.wv'):
        # Charger depuis le cache
        print("\nüì¶ Chargement des tokens depuis le cache...")
        with open(tokens_cache_path, 'rb') as f:
            tokens_data = pickle.load(f)
        tokens = tokens_data['tokens']
        print(f"‚úÖ {len(tokens)} tokens charg√©s depuis le cache")
        
        print("\nüì¶ Chargement du mod√®le Word2Vec depuis le cache...")
        from gensim.models import KeyedVectors
        w2v_vectors = KeyedVectors.load("models/w2v.wv")
        # Cr√©er un objet Word2Vec factice pour la compatibilit√©
        w2v = type('obj', (object,), {'wv': w2v_vectors})()
        print(f"‚úÖ Word2Vec charg√© avec {len(w2v.wv)} mots")
    else:
        # G√©n√©rer et sauvegarder
        print("\nüî§ Tokenisation des textes...")
        tokens = tokenize_corpus(data["complaint_text"])
        
        # Sauvegarder les tokens
        tokens_data = {
            'tokens': tokens,
            'max_length': MAX_LENGTH,
            'w2v_size': W2V_SIZE,
            'timestamp': pd.Timestamp.now().isoformat()
        }
        with open(tokens_cache_path, 'wb') as f:
            pickle.dump(tokens_data, f)
        print(f"üíæ Tokens sauvegard√©s dans {tokens_cache_path}")
        
        # √âTAPE 3 : Cr√©ation des embeddings Word2Vec
        # Apprend √† repr√©senter chaque mot comme un vecteur
        w2v = fit_word2vec(tokens)
    
    # √âTAPE 4 & 5 : Vectorisation et labels (avec cache)
    vectors_cache_path = 'models/vectors_cache.pkl'
    labels_cache_path = 'models/labels_cache.pkl'
    
    if use_cache and os.path.exists(vectors_cache_path) and os.path.exists(labels_cache_path):
        # Charger les vecteurs et labels depuis le cache
        print("\nüì¶ Chargement des vecteurs et labels depuis le cache...")
        
        with open(vectors_cache_path, 'rb') as f:
            X = pickle.load(f)
        print(f"‚úÖ Vecteurs charg√©s : shape {X.shape}")
        
        with open(labels_cache_path, 'rb') as f:
            y = pickle.load(f)
        print(f"‚úÖ Labels charg√©s : shape {y.shape}")
    else:
        # G√©n√©rer et sauvegarder
        # √âTAPE 4 : Vectorisation des textes
        print("\nüéØ Vectorisation des textes...")
        X = text2vec(tokens, w2v)
        
        # Sauvegarder les vecteurs
        with open(vectors_cache_path, 'wb') as f:
            pickle.dump(X, f)
        print(f"üíæ Vecteurs sauvegard√©s dans {vectors_cache_path}")
        
        # √âTAPE 5 : Pr√©paration des labels
        print("\nüè∑Ô∏è Pr√©paration des labels...")
        y = prepare_labels(data, class_mapping)
        
        # Sauvegarder les labels
        with open(labels_cache_path, 'wb') as f:
            pickle.dump(y, f)
        print(f"üíæ Labels sauvegard√©s dans {labels_cache_path}")
    
    # √âTAPE 6 : Division train/test avec stratification
    # 75% pour l'entra√Ænement, 25% pour le test
    # Stratification pour garder les proportions de classes
    from sklearn.model_selection import train_test_split
    
    # Convertir y en indices pour stratification
    y_indices = np.argmax(y, axis=1)
    
    # V√©rifier si stratification possible
    from collections import Counter
    class_counts = Counter(y_indices)
    min_class_count = min(class_counts.values())
    
    if min_class_count >= 2:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=y_indices
        )
        print("‚úÖ Stratification appliqu√©e pour maintenir les proportions")
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42
        )
        print("‚ö†Ô∏è Stratification impossible (classes trop rares)")
    
    # √âTAPE 7 : Cr√©ation et configuration du mod√®le
    print(f"\nü§ñ Cr√©ation du mod√®le pour {num_classes} classes...")
    rnn = create_rnn(num_classes)
    
    # Compilation : d√©finit comment le mod√®le va apprendre
    # Optimiseur avec taux d'apprentissage ajust√© pour 21 classes
    from tensorflow.keras.optimizers import Adam
    optimizer = Adam(learning_rate=0.0005)  # R√©duit de 0.001 pour apprentissage plus stable
    
    rnn.compile(
        optimizer=optimizer,  # Algorithme d'optimisation avec taux ajust√©
        loss="categorical_crossentropy",  # Fonction de perte pour classification multi-classes
        metrics=['categorical_accuracy']  # M√©trique √† surveiller (% de bonnes pr√©dictions)
    )
    
    # Affiche un r√©sum√© de l'architecture du mod√®le
    rnn.summary()
    
    # √âTAPE 8 : Entra√Ænement
    print("\nüèãÔ∏è Entra√Ænement du mod√®le...")
    
    # Callbacks pour le monitoring
    tensorboard_callback = TensorBoard("logs/rnn_complaints_classification")
    
    # Early stopping pour √©viter le surapprentissage
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,  # Augment√© de 3 √† 10 pour laisser plus de temps
        restore_best_weights=True,
        verbose=1  # Affiche quand il s'active
    )
    
    # Checkpoint pour sauvegarder le meilleur mod√®le
    checkpoint = ModelCheckpoint(
        'models/best_model.keras',
        monitor='val_categorical_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    )
    
    # R√©duction du taux d'apprentissage si plateau
    from tensorflow.keras.callbacks import ReduceLROnPlateau
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,  # Divise le LR par 2
        patience=5,   # Apr√®s 5 epochs sans am√©lioration
        min_lr=0.00001,
        verbose=1
    )
    
    # Calculer les poids de classes pour g√©rer le d√©s√©quilibre EXTR√äME
    # Convertir y_train en indices pour compute_class_weight
    y_train_indices = np.argmax(y_train, axis=1)
    unique_classes = np.unique(y_train_indices)
    
    # Compter les √©chantillons par classe
    from collections import Counter
    class_counts = Counter(y_train_indices)
    total_samples = len(y_train_indices)
    n_classes_present = len(unique_classes)
    
    # Calculer les poids avec strat√©gie agressive pour d√©s√©quilibre extr√™me
    class_weight_dict = {}
    max_count = max(class_counts.values())
    
    print("\n‚öñÔ∏è Poids des classes (compensation du d√©s√©quilibre):")
    for class_id in range(num_classes):
        if class_id in class_counts:
            count = class_counts[class_id]
            # Strat√©gie sqrt pour √©viter des poids trop extr√™mes
            # mais toujours favoriser les minorit√©s
            weight = np.sqrt(max_count / count)
            # Limiter les poids pour √©viter l'instabilit√©
            weight = min(weight, 50.0)  # Cap √† 50x
            class_weight_dict[class_id] = weight
            
            if count < 100:  # Classes tr√®s minoritaires
                print(f"  ‚ö†Ô∏è Classe {class_id}: {count} exemples ‚Üí poids {weight:.2f}x")
        else:
            # Classe absente dans train
            class_weight_dict[class_id] = 0.0
            print(f"  ‚ùå Classe {class_id}: ABSENTE dans l'entra√Ænement!")
    
    print(f"\nüìä Classes pr√©sentes: {n_classes_present}/{num_classes}")
    print(f"üìä Poids moyen appliqu√©: {np.mean(list(class_weight_dict.values())):.2f}x")
    
    # Entra√Ænement du mod√®le
    _ = rnn.fit(  # history peut √™tre utilis√© pour analyser l'entra√Ænement
        x=X_train, y=y_train,  # Donn√©es d'entra√Ænement
        validation_data=(X_test, y_test),  # Donn√©es de validation
        epochs=30,  # Augment√© √† 30 pour permettre plus d'apprentissage (21 classes)
        batch_size=64,  # Augment√© √† 64 pour stabiliser l'apprentissage
        class_weight=class_weight_dict,  # Poids pour g√©rer le d√©s√©quilibre
        callbacks=[tensorboard_callback, early_stopping, checkpoint, reduce_lr],  # Pour le monitoring
        verbose=1  # Affiche la progression d√©taill√©e
    )
    
    # √âTAPE 9 : Sauvegarde
    # Sauvegarde le mod√®le entra√Æn√© pour utilisation future
    # Utiliser la variable d'environnement pour le nom du fichier
    output_file = os.getenv('OUTPUT_FILE', 'complaint_classifier')
    model_path = f"models/{output_file}.keras"
    rnn.save(model_path)
    print(f"\n‚úÖ Mod√®le sauvegard√© dans {model_path}")
    print("‚úÖ Meilleur mod√®le sauvegard√© dans models/best_model.keras")
    print("\nüìä Pour visualiser l'entra√Ænement: tensorboard --logdir=logs")

# Point d'entr√©e du script
if __name__ == "__main__":
    # Lance le processus d'entra√Ænement
    main()