"""Script d'entra√Ænement du mod√®le de classification de plaintes pour une compagnie d'assurance.

Ce script :
1. Charge un dataset de plaintes clients avec leurs cat√©gories de produits
2. Transforme les textes en vecteurs num√©riques avec Word2Vec
3. Entra√Æne un r√©seau de neurones (CNN + LSTM) pour pr√©dire la cat√©gorie
4. Sauvegarde les mod√®les entra√Æn√©s pour une utilisation ult√©rieure
"""

import os
from dotenv import load_dotenv

# Charger les variables d'environnement avec override=True pour forcer le rechargement
load_dotenv(override=True)

# Configuration pour r√©duire les messages d'information de TensorFlow
# IMPORTANT : Ces lignes DOIVENT √™tre avant l'import de TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # 3 = Afficher seulement les erreurs (pas les warnings/infos)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # D√©sactive les optimisations oneDNN (r√©duit les messages)

# Imports des biblioth√®ques n√©cessaires
import numpy as np  # Pour les calculs num√©riques et les tableaux
import pandas as pd  # Pour manipuler les donn√©es sous forme de tableaux
import matplotlib.pyplot as plt  # Pour cr√©er des graphiques (pas utilis√© actuellement)
import tensorflow as tf  # Framework de deep learning pour cr√©er le r√©seau de neurones

# Imports des modules sp√©cifiques
from dotenv import load_dotenv  # Pour charger les variables d'environnement depuis le fichier .env
from gensim.models import Word2Vec  # Pour cr√©er des embeddings de mots (Word2Vec)
from keras import layers  # Pour construire les couches du r√©seau de neurones
from spacy.tokenizer import Tokenizer  # Pour d√©couper le texte (pas utilis√© actuellement)
from spacy.lang.en import English  # Pour le traitement du texte en anglais
from sklearn.model_selection import train_test_split  # Pour diviser les donn√©es en train/test
from sklearn.utils.class_weight import compute_class_weight  # Pour g√©rer le d√©s√©quilibre des classes
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint  # Pour le monitoring et la sauvegarde
import json  # Pour lire le fichier d'analyse des classes

# Configuration des hyperparam√®tres (avec valeurs par d√©faut si non d√©finies)
W2V_SIZE = int(os.getenv("W2V_SIZE", 100))  # Dimension des vecteurs Word2Vec
W2V_MIN_COUNT = int(os.getenv("W2V_MIN_COUNT", 3))  # Fr√©quence minimale des mots
MAX_LENGTH = int(os.getenv("MAX_LENGTH", 64))  # Nombre max de mots par commentaire
NB_COMMENT = int(os.getenv("NB_COMMENT", 10000))  # Nombre de commentaires √† utiliser

def tokenize_corpus(texts):
    """Transforme une liste de textes de plaintes en liste de mots (tokens).
    
    Cette fonction pr√©pare le texte pour Word2Vec :
    - Met tout en minuscules pour uniformiser
    - D√©coupe chaque texte en mots individuels
    - Enl√®ve la ponctuation (virgules, points, etc.)
    
    Args:
        texts: Liste de textes de plaintes (textes bruts)
    
    Returns:
        Liste de listes de mots. Chaque sous-liste = les mots d'une plainte
        Exemple: [["credit", "report", "error"], ["mortgage", "payment", "issue"]]
    """
    tokens = []  # Liste qui contiendra tous les textes tokenis√©s
    nlp = English()  # Cr√©er un objet pour analyser l'anglais
    
    for text in texts:
        if pd.isna(text):  # G√©rer les valeurs manquantes
            text = ""
        text = str(text).lower()  # Convertir en minuscules
        # nlp(text) d√©coupe le texte et analyse chaque mot
        # x.text r√©cup√®re le mot, x.is_punct v√©rifie si c'est de la ponctuation
        tokens.append([x.text for x in nlp(text) if not x.is_punct])
    return tokens

def fit_word2vec(tokens):
    """Entra√Æne un mod√®le Word2Vec pour transformer les mots en vecteurs num√©riques.
    
    Word2Vec apprend √† repr√©senter chaque mot comme un vecteur de nombres.
    Les mots similaires auront des vecteurs proches (ex: "excellent" et "super").
    
    Args:
        tokens: Liste de listes de mots provenant de tokenize_corpus
    
    Returns:
        Mod√®le Word2Vec entra√Æn√©
    """
    # Cr√©ation et entra√Ænement du mod√®le Word2Vec
    w2v = Word2Vec(
        sentences=tokens,  # Les commentaires tokenis√©s
        vector_size=W2V_SIZE,  # Taille des vecteurs (100 dimensions par d√©faut)
        min_count=W2V_MIN_COUNT,  # Ignore les mots qui apparaissent moins de 3 fois
        window=5,  # Contexte : regarde 5 mots avant et apr√®s pour apprendre
        workers=2  # Utilise 2 threads pour acc√©l√©rer l'entra√Ænement
    )
    
    # Sauvegarder le mod√®le pour pouvoir le r√©utiliser plus tard
    w2v.wv.save("models/w2v.wv")
    
    # Afficher des statistiques sur le vocabulaire appris
    print(f"\nüìä Informations sur le mod√®le Word2Vec:")
    print(f"- Taille du vocabulaire: {len(w2v.wv)} mots uniques")
    
    return w2v

def text2vec(tokens, w2v):
    """Convertit chaque texte de plainte en une matrice de vecteurs Word2Vec.
    
    Transforme les mots en nombres pour que le r√©seau de neurones puisse les comprendre.
    Chaque commentaire devient une matrice de taille fixe (100 x 64).
    
    Args:
        tokens: Liste de listes de mots
        w2v: Mod√®le Word2Vec entra√Æn√©
    
    Returns:
        Array numpy 3D de forme (nb_commentaires, 100, 64)
        - Dimension 1: Chaque commentaire
        - Dimension 2: Les 100 dimensions du vecteur Word2Vec
        - Dimension 3: Les 64 mots maximum par commentaire
    """
    # Pr√©-allouer le tableau numpy pour √©conomiser la m√©moire
    # Utilisation de float32 au lieu de float64 pour diviser la m√©moire par 2
    n_samples = len(tokens)
    X = np.zeros((n_samples, W2V_SIZE, MAX_LENGTH), dtype=np.float32)
    
    # Pour chaque commentaire tokenis√©
    for i, row_tokens in enumerate(tokens):
        # Remplir la matrice avec les vecteurs des mots (max 64 mots)
        for j in range(min(MAX_LENGTH, len(row_tokens))):
            # V√©rifier si le mot existe dans le vocabulaire
            if row_tokens[j] in w2v.wv:
                # Mettre le vecteur du mot j dans la colonne j
                X[i, :, j] = w2v.wv[row_tokens[j]]
            # Si le mot n'est pas dans le vocabulaire, on laisse des z√©ros
        
        # Afficher la progression tous les 1000 textes
        if i % 1000 == 0:
            print(f"{(i * 100) / len(tokens):.1f}% effectu√©.")
    
    return X

def prepare_labels(data, class_mapping):
    """Pr√©pare les √©tiquettes (labels) pour l'entra√Ænement.
    
    Transforme les cat√©gories de produits en indices num√©riques puis en one-hot encoding.
    
    Args:
        data: DataFrame avec colonne 'product'
        class_mapping: Dictionnaire mappant les noms de classes aux indices
    
    Returns:
        Array one-hot encoded (nb_classes colonnes : une seule vaut 1, les autres 0)
    """
    # Mapper les cat√©gories de produits aux indices num√©riques
    y_indices = data['product'].map(class_mapping)
    
    # Transformer en one-hot encoding
    num_classes = len(class_mapping)
    y = np.zeros((len(y_indices), num_classes))
    for i, idx in enumerate(y_indices):
        if not pd.isna(idx):
            y[i, int(idx)] = 1
    
    return y

def create_rnn(num_classes):
    """Cr√©e l'architecture du r√©seau de neurones pour la classification.
    
    Architecture hybride CNN + LSTM :
    - CNN (Convolution) : D√©tecte des patterns locaux dans le texte
    - LSTM : Comprend les s√©quences et le contexte
    - Dense : Couches finales pour la classification
    
    Returns:
        Mod√®le Keras non compil√©
    """
    return tf.keras.Sequential([
        # Couche d'entr√©e : attend des matrices de taille (100, 64)
        layers.Input(shape=(W2V_SIZE, MAX_LENGTH)),
        
        # Convolution 1D : d√©tecte des patterns de 3 mots cons√©cutifs
        # 64 filtres pour mieux capturer la diversit√© des plaintes
        layers.Convolution1D(64, kernel_size=3, padding='same', activation='relu'),
        
        # MaxPooling : r√©duit la taille de moiti√© en gardant les valeurs importantes
        layers.MaxPool1D(2),
        
        # Convolution suppl√©mentaire pour capturer plus de patterns
        layers.Convolution1D(128, kernel_size=3, padding='same', activation='relu'),
        layers.MaxPool1D(2),
        
        # LSTM : r√©seau r√©current qui comprend les s√©quences
        # 256 neurones pour g√©rer la complexit√© accrue
        layers.LSTM(256, activation="tanh", return_sequences=True),
        
        # Dropout : d√©sactive al√©atoirement 20% des neurones (√©vite le surapprentissage)
        layers.Dropout(0.2),
        
        # Deuxi√®me couche LSTM
        layers.LSTM(128, activation="tanh"),
        
        # Dropout suppl√©mentaire
        layers.Dropout(0.2),
        
        # Dense : couche classique avec 128 neurones
        layers.Dense(128, activation="relu"),
        
        # Dropout avant la sortie
        layers.Dropout(0.1),
        
        # Couche de sortie : autant de neurones que de cat√©gories
        # Softmax garantit que la somme des probabilit√©s = 1
        layers.Dense(num_classes, activation="softmax")
    ])
            
def load_class_mapping():
    """Charge le mapping des classes depuis le fichier d'analyse.
    
    Returns:
        tuple: (class_mapping dict, num_classes int, class_weights dict)
    """
    with open('data/prepared/analyse_result.json', 'r', encoding='utf-8') as f:
        analysis = json.load(f)
    
    # Cr√©er le mapping nom de classe -> indice
    class_mapping = {}
    class_weights = {}
    for i, classe in enumerate(analysis['classes']):
        class_mapping[classe['nom']] = i
        # Utiliser l'inverse du pourcentage comme poids pour compenser le d√©s√©quilibre
        # G√©rer les classes avec pourcentage nul ou tr√®s faible
        pourcentage = max(classe['pourcentage'] / 100.0, 0.0001)  # Minimum 0.01%
        class_weights[i] = 1.0 / pourcentage
    
    # Sauvegarder le mapping pour l'utiliser dans predict.py
    with open('models/class_mapping.json', 'w', encoding='utf-8') as f:
        json.dump({
            'class_mapping': class_mapping,
            'index_to_class': {str(v): k for k, v in class_mapping.items()},
            'num_classes': len(class_mapping)
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìà Classes identifi√©es: {len(class_mapping)} cat√©gories")
    print(f"Cat√©gories les plus fr√©quentes:")
    for classe in analysis['classes'][:5]:
        print(f"  - {classe['nom'][:50]}: {classe['pourcentage']:.2f}%")
    
    return class_mapping, len(class_mapping), class_weights

def main():
    """Fonction principale qui orchestre tout l'entra√Ænement."""
    
    # √âTAPE 0 : Charger le mapping des classes
    class_mapping, num_classes, class_weights = load_class_mapping()
    
    # √âTAPE 1 : Chargement des donn√©es
    # Lit le fichier CSV pr√©par√© par prepare_data.py
    data_path = 'data/prepared/complaints_processed.csv'
    if not os.path.exists(data_path):
        print(f"Erreur: Le fichier {data_path} n'existe pas.")
        print("Veuillez d'abord ex√©cuter prepare_data.py pour pr√©parer les donn√©es.")
        return
    
    print(f"\nüìÇ Chargement des donn√©es depuis {data_path}...")
    print(f"Limite configur√©e: {NB_COMMENT} plaintes")
    
    # Charger seulement le nombre n√©cessaire de lignes
    data = pd.read_csv(data_path, nrows=NB_COMMENT)
    
    # V√©rifier les colonnes
    if 'complaint_text' not in data.columns or 'product' not in data.columns:
        print("Erreur: Le fichier CSV doit contenir les colonnes 'complaint_text' et 'product'")
        return
    
    print(f"Donn√©es charg√©es: {len(data)} plaintes")
    
    # √âTAPE 2 : Pr√©paration du texte
    # Transforme les textes de plaintes en listes de mots
    print("\nüî§ Tokenisation des textes...")
    tokens = tokenize_corpus(data["complaint_text"])
    
    # √âTAPE 3 : Cr√©ation des embeddings Word2Vec
    # Apprend √† repr√©senter chaque mot comme un vecteur
    w2v = fit_word2vec(tokens)
    
    # √âTAPE 4 : Vectorisation des textes
    # Transforme chaque texte en matrice de vecteurs
    print("\nüéØ Vectorisation des textes...")
    X = text2vec(tokens, w2v)
    
    # √âTAPE 5 : Pr√©paration des labels
    # Transforme les cat√©gories en format one-hot
    print("\nüè∑Ô∏è Pr√©paration des labels...")
    y = prepare_labels(data, class_mapping)
    
    # √âTAPE 6 : Division train/test
    # 75% pour l'entra√Ænement, 25% pour le test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    # √âTAPE 7 : Cr√©ation et configuration du mod√®le
    print(f"\nü§ñ Cr√©ation du mod√®le pour {num_classes} classes...")
    rnn = create_rnn(num_classes)
    
    # Compilation : d√©finit comment le mod√®le va apprendre
    rnn.compile(
        optimizer='adam',  # Algorithme d'optimisation (ajuste les poids)
        loss="categorical_crossentropy",  # Fonction de perte pour classification multi-classes
        metrics=['categorical_accuracy']  # M√©trique √† surveiller (% de bonnes pr√©dictions)
    )
    
    # Affiche un r√©sum√© de l'architecture du mod√®le
    rnn.summary()
    
    # √âTAPE 8 : Entra√Ænement
    print("\nüèãÔ∏è Entra√Ænement du mod√®le...")
    
    # Callbacks pour le monitoring
    tensorboard_callback = TensorBoard("logs/rnn_complaints_classification")
    
    # Early stopping pour √©viter le surapprentissage
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
    
    # Checkpoint pour sauvegarder le meilleur mod√®le
    checkpoint = ModelCheckpoint(
        'models/best_model.keras',
        monitor='val_categorical_accuracy',
        save_best_only=True,
        mode='max'
    )
    
    # Calculer les poids de classes pour g√©rer le d√©s√©quilibre
    # Convertir y_train en indices pour compute_class_weight
    y_train_indices = np.argmax(y_train, axis=1)
    unique_classes = np.unique(y_train_indices)
    class_weight_array = compute_class_weight(
        'balanced',
        classes=unique_classes,
        y=y_train_indices
    )
    class_weight_dict = {i: weight for i, weight in enumerate(class_weight_array)}
    
    # Entra√Ænement du mod√®le
    history = rnn.fit(
        x=X_train, y=y_train,  # Donn√©es d'entra√Ænement
        validation_data=(X_test, y_test),  # Donn√©es de validation
        epochs=20,  # Nombre de passes sur les donn√©es (augment√© pour plus de classes)
        batch_size=32,  # Traite 32 textes √† la fois
        class_weight=class_weight_dict,  # Poids pour g√©rer le d√©s√©quilibre
        callbacks=[tensorboard_callback, early_stopping, checkpoint]  # Pour le monitoring
    )
    
    # √âTAPE 9 : Sauvegarde
    # Sauvegarde le mod√®le entra√Æn√© pour utilisation future
    # Utiliser la variable d'environnement pour le nom du fichier
    output_file = os.getenv('OUTPUT_FILE', 'complaint_classifier')
    model_path = f"models/{output_file}.keras"
    rnn.save(model_path)
    print(f"\n‚úÖ Mod√®le sauvegard√© dans {model_path}")
    print("‚úÖ Meilleur mod√®le sauvegard√© dans models/best_model.keras")
    print("\nüìä Pour visualiser l'entra√Ænement: tensorboard --logdir=logs")

# Point d'entr√©e du script
if __name__ == "__main__":
    # Lance le processus d'entra√Ænement
    main()