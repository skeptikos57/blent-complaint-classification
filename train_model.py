"""Script d'entraÃ®nement du modÃ¨le de classification de plaintes pour une compagnie d'assurance.

Ce script :
1. Charge un dataset de plaintes clients avec leurs catÃ©gories de produits
2. Transforme les textes en vecteurs numÃ©riques avec Word2Vec
3. EntraÃ®ne un rÃ©seau de neurones (CNN + LSTM) pour prÃ©dire la catÃ©gorie
4. Sauvegarde les modÃ¨les entraÃ®nÃ©s pour une utilisation ultÃ©rieure
"""

import os
from dotenv import load_dotenv

# Charger les variables d'environnement avec override=True pour forcer le rechargement
load_dotenv(override=True)

# Configuration pour rÃ©duire les messages d'information de TensorFlow
# IMPORTANT : Ces lignes DOIVENT Ãªtre avant l'import de TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # 3 = Afficher seulement les erreurs (pas les warnings/infos)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # DÃ©sactive les optimisations oneDNN (rÃ©duit les messages)

# Imports des bibliothÃ¨ques nÃ©cessaires
import numpy as np  # Pour les calculs numÃ©riques et les tableaux
import pandas as pd  # Pour manipuler les donnÃ©es sous forme de tableaux
import matplotlib.pyplot as plt  # Pour crÃ©er des graphiques (pas utilisÃ© actuellement)
import tensorflow as tf  # Framework de deep learning pour crÃ©er le rÃ©seau de neurones

# Imports des modules spÃ©cifiques
from dotenv import load_dotenv  # Pour charger les variables d'environnement depuis le fichier .env
from gensim.models import Word2Vec  # Pour crÃ©er des embeddings de mots (Word2Vec)
from keras import layers  # Pour construire les couches du rÃ©seau de neurones
from spacy.tokenizer import Tokenizer  # Pour dÃ©couper le texte (pas utilisÃ© actuellement)
from spacy.lang.en import English  # Pour le traitement du texte en anglais
from sklearn.model_selection import train_test_split  # Pour diviser les donnÃ©es en train/test
from sklearn.utils.class_weight import compute_class_weight  # Pour gÃ©rer le dÃ©sÃ©quilibre des classes
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint  # Pour le monitoring et la sauvegarde
import json  # Pour lire le fichier d'analyse des classes

# Configuration des hyperparamÃ¨tres (avec valeurs par dÃ©faut si non dÃ©finies)
W2V_SIZE = int(os.getenv("W2V_SIZE", 100))  # Dimension des vecteurs Word2Vec
W2V_MIN_COUNT = int(os.getenv("W2V_MIN_COUNT", 3))  # FrÃ©quence minimale des mots
MAX_LENGTH = int(os.getenv("MAX_LENGTH", 64))  # Nombre max de mots par commentaire
NB_COMMENT = int(os.getenv("NB_COMMENT", 10000))  # Nombre de commentaires Ã  utiliser

def tokenize_corpus(texts):
    """Transforme une liste de textes de plaintes en liste de mots (tokens).
    
    Cette fonction prÃ©pare le texte pour Word2Vec :
    - Met tout en minuscules pour uniformiser
    - DÃ©coupe chaque texte en mots individuels
    - EnlÃ¨ve la ponctuation (virgules, points, etc.)
    
    Args:
        texts: Liste de textes de plaintes (textes bruts)
    
    Returns:
        Liste de listes de mots. Chaque sous-liste = les mots d'une plainte
        Exemple: [["credit", "report", "error"], ["mortgage", "payment", "issue"]]
    """
    tokens = []  # Liste qui contiendra tous les textes tokenisÃ©s
    nlp = English()  # CrÃ©er un objet pour analyser l'anglais
    
    for text in texts:
        if pd.isna(text):  # GÃ©rer les valeurs manquantes
            text = ""
        text = str(text).lower()  # Convertir en minuscules
        # nlp(text) dÃ©coupe le texte et analyse chaque mot
        # x.text rÃ©cupÃ¨re le mot, x.is_punct vÃ©rifie si c'est de la ponctuation
        tokens.append([x.text for x in nlp(text) if not x.is_punct])
    return tokens

def fit_word2vec(tokens):
    """EntraÃ®ne un modÃ¨le Word2Vec pour transformer les mots en vecteurs numÃ©riques.
    
    Word2Vec apprend Ã  reprÃ©senter chaque mot comme un vecteur de nombres.
    Les mots similaires auront des vecteurs proches (ex: "excellent" et "super").
    
    Args:
        tokens: Liste de listes de mots provenant de tokenize_corpus
    
    Returns:
        ModÃ¨le Word2Vec entraÃ®nÃ©
    """
    # CrÃ©ation et entraÃ®nement du modÃ¨le Word2Vec
    w2v = Word2Vec(
        sentences=tokens,  # Les commentaires tokenisÃ©s
        vector_size=W2V_SIZE,  # Taille des vecteurs (100 dimensions par dÃ©faut)
        min_count=W2V_MIN_COUNT,  # Ignore les mots qui apparaissent moins de 3 fois
        window=5,  # Contexte : regarde 5 mots avant et aprÃ¨s pour apprendre
        workers=2  # Utilise 2 threads pour accÃ©lÃ©rer l'entraÃ®nement
    )
    
    # Sauvegarder le modÃ¨le pour pouvoir le rÃ©utiliser plus tard
    w2v.wv.save("models/w2v.wv")
    
    # Afficher des statistiques sur le vocabulaire appris
    print(f"\nğŸ“Š Informations sur le modÃ¨le Word2Vec:")
    print(f"- Taille du vocabulaire: {len(w2v.wv)} mots uniques")
    
    return w2v

def text2vec(tokens, w2v):
    """Convertit chaque texte de plainte en une matrice de vecteurs Word2Vec.
    
    Transforme les mots en nombres pour que le rÃ©seau de neurones puisse les comprendre.
    Chaque commentaire devient une matrice de taille fixe (100 x 64).
    
    Args:
        tokens: Liste de listes de mots
        w2v: ModÃ¨le Word2Vec entraÃ®nÃ©
    
    Returns:
        Array numpy 3D de forme (nb_commentaires, 100, 64)
        - Dimension 1: Chaque commentaire
        - Dimension 2: Les 100 dimensions du vecteur Word2Vec
        - Dimension 3: Les 64 mots maximum par commentaire
    """
    # PrÃ©-allouer le tableau numpy pour Ã©conomiser la mÃ©moire
    # Utilisation de float32 au lieu de float64 pour diviser la mÃ©moire par 2
    n_samples = len(tokens)
    X = np.zeros((n_samples, W2V_SIZE, MAX_LENGTH), dtype=np.float32)
    
    # Pour chaque commentaire tokenisÃ©
    for i, row_tokens in enumerate(tokens):
        # Remplir la matrice avec les vecteurs des mots (max 64 mots)
        for j in range(min(MAX_LENGTH, len(row_tokens))):
            # VÃ©rifier si le mot existe dans le vocabulaire
            if row_tokens[j] in w2v.wv:
                # Mettre le vecteur du mot j dans la colonne j
                X[i, :, j] = w2v.wv[row_tokens[j]]
            # Si le mot n'est pas dans le vocabulaire, on laisse des zÃ©ros
        
        # Afficher la progression tous les 1000 textes
        if i % 1000 == 0:
            print(f"{(i * 100) / len(tokens):.1f}% effectuÃ©.")
    
    return X

def prepare_labels(data, class_mapping):
    """PrÃ©pare les Ã©tiquettes (labels) pour l'entraÃ®nement.
    
    Transforme les catÃ©gories de produits en indices numÃ©riques puis en one-hot encoding.
    
    Args:
        data: DataFrame avec colonne 'product'
        class_mapping: Dictionnaire mappant les noms de classes aux indices
    
    Returns:
        Array one-hot encoded (nb_classes colonnes : une seule vaut 1, les autres 0)
    """
    # Mapper les catÃ©gories de produits aux indices numÃ©riques
    y_indices = data['product'].map(class_mapping)
    
    # Transformer en one-hot encoding
    num_classes = len(class_mapping)
    y = np.zeros((len(y_indices), num_classes))
    for i, idx in enumerate(y_indices):
        if not pd.isna(idx):
            y[i, int(idx)] = 1
    
    return y

def create_rnn(num_classes):
    """CrÃ©e l'architecture du rÃ©seau de neurones pour la classification.
    
    Architecture hybride CNN + LSTM :
    - CNN (Convolution) : DÃ©tecte des patterns locaux dans le texte
    - LSTM : Comprend les sÃ©quences et le contexte
    - Dense : Couches finales pour la classification
    
    Returns:
        ModÃ¨le Keras non compilÃ©
    """
    return tf.keras.Sequential([
        # Couche d'entrÃ©e : attend des matrices de taille (100, 64)
        layers.Input(shape=(W2V_SIZE, MAX_LENGTH)),
        
        # Convolution 1D : dÃ©tecte des patterns de 3 mots consÃ©cutifs
        # 64 filtres pour mieux capturer la diversitÃ© des plaintes
        layers.Convolution1D(64, kernel_size=3, padding='same', activation='relu'),
        
        # MaxPooling : rÃ©duit la taille de moitiÃ© en gardant les valeurs importantes
        layers.MaxPool1D(2),
        
        # Convolution supplÃ©mentaire pour capturer plus de patterns
        layers.Convolution1D(128, kernel_size=3, padding='same', activation='relu'),
        layers.MaxPool1D(2),
        
        # LSTM : rÃ©seau rÃ©current qui comprend les sÃ©quences
        # 256 neurones pour gÃ©rer la complexitÃ© accrue
        layers.LSTM(256, activation="tanh", return_sequences=True),
        
        # Dropout : dÃ©sactive alÃ©atoirement 20% des neurones (Ã©vite le surapprentissage)
        layers.Dropout(0.2),
        
        # DeuxiÃ¨me couche LSTM
        layers.LSTM(128, activation="tanh"),
        
        # Dropout supplÃ©mentaire
        layers.Dropout(0.2),
        
        # Dense : couche classique avec 128 neurones
        layers.Dense(128, activation="relu"),
        
        # Dropout avant la sortie
        layers.Dropout(0.1),
        
        # Couche de sortie : autant de neurones que de catÃ©gories
        # Softmax garantit que la somme des probabilitÃ©s = 1
        layers.Dense(num_classes, activation="softmax")
    ])
            
def load_class_mapping():
    """Charge le mapping des classes depuis le fichier d'analyse.
    
    Returns:
        tuple: (class_mapping dict, num_classes int, class_weights dict)
    """
    with open('data/prepared/analyse_result.json', 'r', encoding='utf-8') as f:
        analysis = json.load(f)
    
    # CrÃ©er le mapping nom de classe -> indice
    class_mapping = {}
    class_weights = {}
    for i, classe in enumerate(analysis['classes']):
        class_mapping[classe['nom']] = i
        # Utiliser l'inverse du pourcentage comme poids pour compenser le dÃ©sÃ©quilibre
        # GÃ©rer les classes avec pourcentage nul ou trÃ¨s faible
        pourcentage = max(classe['pourcentage'] / 100.0, 0.0001)  # Minimum 0.01%
        class_weights[i] = 1.0 / pourcentage
    
    # Sauvegarder le mapping pour l'utiliser dans predict.py
    with open('models/class_mapping.json', 'w', encoding='utf-8') as f:
        json.dump({
            'class_mapping': class_mapping,
            'index_to_class': {str(v): k for k, v in class_mapping.items()},
            'num_classes': len(class_mapping)
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ˆ Classes identifiÃ©es: {len(class_mapping)} catÃ©gories")
    print(f"CatÃ©gories les plus frÃ©quentes:")
    for classe in analysis['classes'][:5]:
        print(f"  - {classe['nom'][:50]}: {classe['pourcentage']:.2f}%")
    
    return class_mapping, len(class_mapping), class_weights

def main():
    """Fonction principale qui orchestre tout l'entraÃ®nement."""
    
    # Ã‰TAPE 0 : Charger le mapping des classes
    class_mapping, num_classes, class_weights = load_class_mapping()
    
    # Ã‰TAPE 1 : Chargement des donnÃ©es
    # Lit le fichier CSV prÃ©parÃ© par prepare_data.py
    data_path = 'data/prepared/complaints_processed.csv'
    if not os.path.exists(data_path):
        print(f"Erreur: Le fichier {data_path} n'existe pas.")
        print("Veuillez d'abord exÃ©cuter prepare_data.py pour prÃ©parer les donnÃ©es.")
        return
    
    print(f"\nğŸ“‚ Chargement des donnÃ©es depuis {data_path}...")
    print(f"Limite configurÃ©e: {NB_COMMENT} plaintes")
    
    # Charger seulement le nombre nÃ©cessaire de lignes
    data = pd.read_csv(data_path, nrows=NB_COMMENT)
    
    # VÃ©rifier les colonnes
    if 'complaint_text' not in data.columns or 'product' not in data.columns:
        print("Erreur: Le fichier CSV doit contenir les colonnes 'complaint_text' et 'product'")
        return
    
    print(f"DonnÃ©es chargÃ©es: {len(data)} plaintes")
    
    # Ã‰TAPE 2 : PrÃ©paration du texte
    # VÃ©rifier si on doit utiliser le cache ou regÃ©nÃ©rer
    import pickle
    import sys
    
    use_cache = True
    if len(sys.argv) > 1 and sys.argv[1] == '--regenerate':
        use_cache = False
        print("ğŸ”„ Mode regÃ©nÃ©ration : crÃ©ation de nouveaux tokens et Word2Vec")
    
    tokens_cache_path = 'models/tokens_cache.pkl'
    
    if use_cache and os.path.exists(tokens_cache_path) and os.path.exists('models/w2v.wv'):
        # Charger depuis le cache
        print("\nğŸ“¦ Chargement des tokens depuis le cache...")
        with open(tokens_cache_path, 'rb') as f:
            tokens_data = pickle.load(f)
        tokens = tokens_data['tokens']
        print(f"âœ… {len(tokens)} tokens chargÃ©s depuis le cache")
        
        print("\nğŸ“¦ Chargement du modÃ¨le Word2Vec depuis le cache...")
        from gensim.models import KeyedVectors
        w2v_vectors = KeyedVectors.load("models/w2v.wv")
        # CrÃ©er un objet Word2Vec factice pour la compatibilitÃ©
        w2v = type('obj', (object,), {'wv': w2v_vectors})()
        print(f"âœ… Word2Vec chargÃ© avec {len(w2v.wv)} mots")
    else:
        # GÃ©nÃ©rer et sauvegarder
        print("\nğŸ”¤ Tokenisation des textes...")
        tokens = tokenize_corpus(data["complaint_text"])
        
        # Sauvegarder les tokens
        tokens_data = {
            'tokens': tokens,
            'max_length': MAX_LENGTH,
            'w2v_size': W2V_SIZE,
            'timestamp': pd.Timestamp.now().isoformat()
        }
        with open(tokens_cache_path, 'wb') as f:
            pickle.dump(tokens_data, f)
        print(f"ğŸ’¾ Tokens sauvegardÃ©s dans {tokens_cache_path}")
        
        # Ã‰TAPE 3 : CrÃ©ation des embeddings Word2Vec
        # Apprend Ã  reprÃ©senter chaque mot comme un vecteur
        w2v = fit_word2vec(tokens)
    
    # Ã‰TAPE 4 & 5 : Vectorisation et labels (avec cache)
    vectors_cache_path = 'models/vectors_cache.pkl'
    labels_cache_path = 'models/labels_cache.pkl'
    
    if use_cache and os.path.exists(vectors_cache_path) and os.path.exists(labels_cache_path):
        # Charger les vecteurs et labels depuis le cache
        print("\nğŸ“¦ Chargement des vecteurs et labels depuis le cache...")
        
        with open(vectors_cache_path, 'rb') as f:
            X = pickle.load(f)
        print(f"âœ… Vecteurs chargÃ©s : shape {X.shape}")
        
        with open(labels_cache_path, 'rb') as f:
            y = pickle.load(f)
        print(f"âœ… Labels chargÃ©s : shape {y.shape}")
    else:
        # GÃ©nÃ©rer et sauvegarder
        # Ã‰TAPE 4 : Vectorisation des textes
        print("\nğŸ¯ Vectorisation des textes...")
        X = text2vec(tokens, w2v)
        
        # Sauvegarder les vecteurs
        with open(vectors_cache_path, 'wb') as f:
            pickle.dump(X, f)
        print(f"ğŸ’¾ Vecteurs sauvegardÃ©s dans {vectors_cache_path}")
        
        # Ã‰TAPE 5 : PrÃ©paration des labels
        print("\nğŸ·ï¸ PrÃ©paration des labels...")
        y = prepare_labels(data, class_mapping)
        
        # Sauvegarder les labels
        with open(labels_cache_path, 'wb') as f:
            pickle.dump(y, f)
        print(f"ğŸ’¾ Labels sauvegardÃ©s dans {labels_cache_path}")
    
    # Ã‰TAPE 6 : Division train/test
    # 75% pour l'entraÃ®nement, 25% pour le test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    # Ã‰TAPE 7 : CrÃ©ation et configuration du modÃ¨le
    print(f"\nğŸ¤– CrÃ©ation du modÃ¨le pour {num_classes} classes...")
    rnn = create_rnn(num_classes)
    
    # Compilation : dÃ©finit comment le modÃ¨le va apprendre
    rnn.compile(
        optimizer='adam',  # Algorithme d'optimisation (ajuste les poids)
        loss="categorical_crossentropy",  # Fonction de perte pour classification multi-classes
        metrics=['categorical_accuracy']  # MÃ©trique Ã  surveiller (% de bonnes prÃ©dictions)
    )
    
    # Affiche un rÃ©sumÃ© de l'architecture du modÃ¨le
    rnn.summary()
    
    # Ã‰TAPE 8 : EntraÃ®nement
    print("\nğŸ‹ï¸ EntraÃ®nement du modÃ¨le...")
    
    # Callbacks pour le monitoring
    tensorboard_callback = TensorBoard("logs/rnn_complaints_classification")
    
    # Early stopping pour Ã©viter le surapprentissage
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
    
    # Checkpoint pour sauvegarder le meilleur modÃ¨le
    checkpoint = ModelCheckpoint(
        'models/best_model.keras',
        monitor='val_categorical_accuracy',
        save_best_only=True,
        mode='max'
    )
    
    # Calculer les poids de classes pour gÃ©rer le dÃ©sÃ©quilibre
    # Convertir y_train en indices pour compute_class_weight
    y_train_indices = np.argmax(y_train, axis=1)
    unique_classes = np.unique(y_train_indices)
    class_weight_array = compute_class_weight(
        'balanced',
        classes=unique_classes,
        y=y_train_indices
    )
    class_weight_dict = {i: weight for i, weight in enumerate(class_weight_array)}
    
    # EntraÃ®nement du modÃ¨le
    _ = rnn.fit(  # history peut Ãªtre utilisÃ© pour analyser l'entraÃ®nement
        x=X_train, y=y_train,  # DonnÃ©es d'entraÃ®nement
        validation_data=(X_test, y_test),  # DonnÃ©es de validation
        epochs=20,  # Nombre de passes sur les donnÃ©es (augmentÃ© pour plus de classes)
        batch_size=32,  # Traite 32 textes Ã  la fois
        class_weight=class_weight_dict,  # Poids pour gÃ©rer le dÃ©sÃ©quilibre
        callbacks=[tensorboard_callback, early_stopping, checkpoint]  # Pour le monitoring
    )
    
    # Ã‰TAPE 9 : Sauvegarde
    # Sauvegarde le modÃ¨le entraÃ®nÃ© pour utilisation future
    # Utiliser la variable d'environnement pour le nom du fichier
    output_file = os.getenv('OUTPUT_FILE', 'complaint_classifier')
    model_path = f"models/{output_file}.keras"
    rnn.save(model_path)
    print(f"\nâœ… ModÃ¨le sauvegardÃ© dans {model_path}")
    print("âœ… Meilleur modÃ¨le sauvegardÃ© dans models/best_model.keras")
    print("\nğŸ“Š Pour visualiser l'entraÃ®nement: tensorboard --logdir=logs")

# Point d'entrÃ©e du script
if __name__ == "__main__":
    # Lance le processus d'entraÃ®nement
    main()